# -*- coding: utf-8 -*-
"""a4_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bcLY0PeVQ81dfep2kKcfU54QeJwqtqx0

# Basic Setup 
## Transforming the datafiles into correct format
"""

import numpy as np
import torch
import pandas as pd

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("Using {}".format(device))

base = "/content/drive/MyDrive/"
from google.colab import drive
drive.mount('/content/drive')

df_train = pd.read_csv(base+"train.csv")
df_test = pd.read_csv(base+"public_test.csv")

iter = 1
x_final = np.zeros(4)
y_final = np.zeros(4)
for index, row in df_train.iterrows():
    x = row[1:]
    npx = x.to_numpy()
    npx = npx.reshape((48,48))
    arr = npx
    use=np.zeros(1)
    use[0]=row[0]
    if iter==1:
      x_final = arr
      y_final = use
      iter = iter+1
    else:
      x_final = np.dstack([x_final, arr])
      y_final = np.vstack([y_final,use])
      iter = iter +1
    print(iter)

iter = 1
x_final_t = np.zeros(4)
y_final_t = np.zeros(4)
for index, row in df_test.iterrows():
    x = row[1:]
    npx = x.to_numpy()
    npx = npx.reshape((48,48))
    arr = npx
    use = np.zeros(1)
    use[0]=row[0]
    if iter==1:
      x_final_t = arr
      y_final_t = use
      iter = iter+1
    else:
      x_final_t = np.dstack([x_final_t, arr])
      y_final_t = np.vstack([y_final_t,use])
      iter = iter+1
    print(iter)

x_final = np.swapaxes(x_final,1,2)
x_final = np.swapaxes(x_final,0,1)
x_final_t = np.swapaxes(x_final_t,1,2)
x_final_t = np.swapaxes(x_final_t,0,1)

np.save(base+"x_final", x_final)
np.save(base+"x_final_t", x_final_t)
np.save(base+"y_final", y_final)
np.save(base+"y_final_t", y_final_t)

x_final=np.load(base+"x_final.npy")
x_final_t=np.load(base+"x_final_t.npy")
y_final=np.load(base+"y_final.npy")
y_final_t=np.load(base+"y_final_t.npy")

print(np.shape(x_final))
print(np.shape(x_final_t))
print(np.shape(y_final))
print(np.shape(y_final_t))

"""# Datset"""

from torch.utils.data import Dataset

class data(Dataset):
    def __init__(self, train):
        if train:
            self.data= x_final               # in this just define dataset of train and test after split into self.data
            self.target = y_final
        else:
            self.data = x_final_t
            self.target = y_final_t            

    def __getitem__(self, index):
        target = int(self.target[index])                                                      # gets data point sequentially
        x = self.data[index]
        return x, target

    def __len__(self):
        return len(self.data)                                                      # gets nmber of datapoints

"""# Testing exact configs of network"""

one_dp = x_final[0]

"""# Model Acrhitecture"""

from torch import nn
import torch.nn.functional as func
from torchsummary import summary

class cnn(nn.Module):   # Inheritance in Python cnn inherits nn.Module 
  def __init__(self):   # constructor
    super(cnn, self).__init__()
    self.conv1 = nn.Conv2d(1,64,3,3,0) 
    self.pool1 = nn.AvgPool2d(2,2,0)
    self.conv2 = nn.Conv2d(64,128,2,2,0)
    self.pool2 = nn.AvgPool2d(2,2,0)
    self.fc1 = nn.Linear(512,100)
    self.fc2 = nn.Linear(100,7)
    self.bn1 = nn.BatchNorm2d(64)
    self.bn2 = nn.BatchNorm2d(128)
    self.bn3 = nn.BatchNorm1d(100)
    self.bn4 = nn.BatchNorm1d(7)

  def forward(self,x):
    x = torch.unsqueeze(x,1)      ## This is because cnn input is (batch_size * inp_channels * heigh * width)
                                  ## from my dataloader the input comes of the form (height*width)
                                  ## so I unsqueeze to add one dimension 
                                  ## and then first dimension is batch size
    ol1 = self.bn1(func.relu(self.conv1(x)))
    ol1 = self.pool1(ol1)
    ol1 = self.bn2(func.relu(self.conv2(ol1)))
    ol1 = self.pool2(ol1)
    ol1 = ol1.view(-1,512)         ## Flattening the input
    ol1 = self.bn3(func.relu(self.fc1(ol1)))
    ol1 = self.bn4(func.relu(self.fc2(ol1)))
    output = func.softmax(ol1,dim=1)
    return output

model = cnn()
model.to(device)
summary(model,input_size=(48,48))

"""# Model Training"""

import torch
import torch.nn as nn
import torch.optim as opti
import torch.nn.functional as func
import torch.backends.cudnn as cudnn
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import Dataset, DataLoader

def visualization(loss_arr,epo):
  x = np.linspace(1,epo,epo)
  plt.plot(x,loss_arr)

def training(lr, epochs):
  print('==> Preparing data..')
  trainset = data(train=True)
  trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)
  testset = data(train=False)
  testloader = DataLoader(testset, batch_size=1, shuffle=True, num_workers=2)

  print('==> Building model..')
  nnet = cnn()
  nnet = nnet.to(device)
  if device == 'cuda':
      nnet = torch.nn.DataParallel(nnet)                                           ## for using multiple GPUs
      cudnn.benchmark = True
  
  criterion = nn.CrossEntropyLoss()
  optimizer = opti.SGD(nnet.parameters(), lr) 
  trainloss =[]
  for epoch in range(epochs):
    l= train_single_epoch(lr,criterion,optimizer,device,nnet,trainloader,128)
    trainloss.append(l)
    print("The loss was {}".format(l))
  visualization(trainloss,epochs)

  print("==> Testing the model..")
  print("Accuracy is found to be {}".format(test(testloader,criterion,nnet,device)))

def train_single_epoch(lr,criterion,optimizer,device,nnet,trainloader,bsize):
  nnet.train()                        ## a pytorch routine it is to turn ON some advanced layers which were turned OFF during testing  
  tloss =0
  for index,(x,y) in enumerate(trainloader):
    x= x.to(device).float()
    y = y.to(device).long()
    optimizer.zero_grad()          ## zeroing the gradients to nullify the effects of previous iterations
    pred_y = nnet(x)
    loss = criterion(pred_y,y)
    loss.backward()
    optimizer.step()
    tloss += loss.item() 
  return (tloss/index)

def test(testloader,criterion,nnet,device):
  nnet.eval()                     ## a pytorch routine it is to turn OFF some advanced layers during testing   
  correct = 0
  total = 0
  with torch.no_grad():          ## turn OFF autograd
    for index,(x,y) in enumerate(testloader):
      total = total + 1
      x= x.to(device).float()
      y = y.to(device).long()
      pred_ysf = nnet(x)
      pred_y = torch.max(pred_ysf,1)[1][0]
      if (pred_y) == (y) :
        correct = correct +1
  return (correct/total)

training(0.1,100)